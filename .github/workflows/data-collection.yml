name: Data Collection

# This workflow collects environmental data for Ontario.
#
# Usage:
#   1. Go to Actions > Data Collection > Run workflow
#   2. Select "collect" mode (default)
#   3. Check which data sources you want to collect
#   4. Click "Run workflow"
#
# By default, Williams Treaty Communities and Provincial Parks are selected.
# You can select additional data sources or deselect the defaults as needed.

on:
  workflow_dispatch:
    inputs:
      mode:
        description: 'Mode: status-only | collect-all | collect-selected'
        required: true
        type: choice
        options:
          - status-only
          - collect-all
          - collect-selected
        default: status-only
      datasets:
        description: 'Dataset IDs to collect (one per line or comma-separated). Run "status-only" first to see available datasets.'
        type: string
        required: false
        default: ''
      overwrite:
        description: 'Overwrite existing datasets? If unchecked, will skip datasets that already exist in artifacts.'
        type: boolean
        required: false
        default: false

jobs:
  check-status:
    name: Check Data Status
    runs-on: ubuntu-latest
    outputs:
      status: ${{ steps.check.outputs.status }}

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"

    - name: List available datasets from registry
      run: |
        echo "## üìã Available Datasets" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "The following datasets are available in the registry:" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        python3 << 'PYTHON_EOF'
        from ontario_data.datasets import get_enabled_datasets, DATASETS
        import os

        print("### Enabled Datasets")
        print("")
        for dataset in get_enabled_datasets():
            print(f"- `{dataset.id}` - {dataset.name}")

        print("")
        print("### Disabled Datasets")
        print("")
        for dataset_id, dataset in DATASETS.items():
            if not dataset.enabled:
                print(f"- ~~`{dataset_id}`~~ - {dataset.name} (disabled)")

        # Also write to GITHUB_STEP_SUMMARY
        with open(os.getenv('GITHUB_STEP_SUMMARY'), 'a') as f:
            f.write("### Enabled Datasets\n\n")
            for dataset in get_enabled_datasets():
                f.write(f"- `{dataset.id}` - {dataset.name}\n")
            f.write("\n### Disabled Datasets\n\n")
            for dataset_id, dataset in DATASETS.items():
                if not dataset.enabled:
                    f.write(f"- ~~`{dataset_id}`~~ - {dataset.name} (disabled)\n")
        PYTHON_EOF

    - name: Check existing data status
      id: check
      run: |
        # Run status check but don't fail - this is just informational
        set +e  # Don't exit on error
        python check_data_status.py
        # Ignore exit code - missing data before collection is expected

        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## üìä Current Data Status" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "Status check completed. Missing data is expected before collection." >> $GITHUB_STEP_SUMMARY

        # Always succeed - this is just informational
        exit 0

  collect-data:
    name: Collect Selected Data
    needs: check-status
    if: github.event.inputs.mode == 'collect-all' || github.event.inputs.mode == 'collect-selected'
    runs-on: ubuntu-latest
    timeout-minutes: 60

    steps:
    - uses: actions/checkout@v4

    - name: Show collection mode
      run: |
        echo "Collection mode: ${{ github.event.inputs.mode }}"
        if [ "${{ github.event.inputs.mode }}" == "collect-selected" ]; then
          echo "Selected datasets: ${{ github.event.inputs.datasets }}"
        fi

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"

    - name: Download previous data artifact
      continue-on-error: true
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');

          // Find the most recent successful Data Collection run
          const runs = await github.rest.actions.listWorkflowRuns({
            owner: context.repo.owner,
            repo: context.repo.repo,
            workflow_id: 'data-collection.yml',
            status: 'completed',
            per_page: 5
          });

          if (runs.data.workflow_runs.length === 0) {
            console.log('No previous collection runs found - starting fresh');
            return;
          }

          // Look for artifacts in recent runs
          for (const run of runs.data.workflow_runs) {
            const artifacts = await github.rest.actions.listWorkflowRunArtifacts({
              owner: context.repo.owner,
              repo: context.repo.repo,
              run_id: run.id,
            });

            const dataArtifact = artifacts.data.artifacts.find(a =>
              a.name.startsWith("ontario-data-")
            );

            if (dataArtifact) {
              console.log(`Found previous artifact: ${dataArtifact.name} from run ${run.run_number}`);

              const download = await github.rest.actions.downloadArtifact({
                owner: context.repo.owner,
                repo: context.repo.repo,
                artifact_id: dataArtifact.id,
                archive_format: 'zip',
              });

              fs.writeFileSync('${{github.workspace}}/previous-data.zip', Buffer.from(download.data));
              console.log('Downloaded previous data artifact');
              return;
            }
          }

          console.log('No previous data artifacts found - starting fresh');

    - name: Extract previous data
      continue-on-error: true
      run: |
        if [ -f previous-data.zip ]; then
          echo "Extracting previous data artifact..."
          unzip -o previous-data.zip -d .
          echo "Previous data extracted:"
          find data -type f 2>/dev/null | head -20 || echo "No previous data files"
        else
          echo "No previous data to extract - starting fresh"
        fi

    - name: Check which datasets already exist
      id: check_existing
      run: |
        echo "Checking which datasets already exist in artifacts..."

        # Create a summary of existing datasets
        existing_datasets=""

        if [ "${{ github.event.inputs.overwrite }}" == "true" ]; then
          echo "‚ö†Ô∏è  OVERWRITE mode enabled - will re-collect all selected datasets"
        else
          echo "Checking for existing datasets to skip..."

          python3 << 'PYTHON_EOF'
        import os
        import sys
        from pathlib import Path

        sys.path.insert(0, os.getcwd())
        from ontario_data.datasets import DATASETS

        existing = []
        for dataset_id, dataset in DATASETS.items():
            if not dataset.enabled or not dataset.output_path:
                continue

            if Path(dataset.output_path).exists():
                existing.append(dataset_id)
                print(f"  ‚úì {dataset_id}: exists, will SKIP unless overwrite=true")
                # Set env var to skip this dataset
                env_var = f'SKIP_{dataset_id.upper()}'
                with open(os.getenv('GITHUB_ENV'), 'a') as f:
                    f.write(f'{env_var}=true\n')
            else:
                print(f"  ‚úó {dataset_id}: missing, will collect if selected")

        print(f"\n{len(existing)} datasets already exist and will be skipped")
        PYTHON_EOF
        fi

    - name: Prepare data directories
      run: |
        # Create necessary directories
        mkdir -p data/raw data/processed

    - name: Check before status
      run: |
        echo "=== DATA STATUS BEFORE COLLECTION ==="
        python check_data_status.py || echo "No data status check available yet"

    - name: Download missing raw data files
      run: |
        # Download required CSV files if they don't exist
        # Note: Water advisories CSV - check if exists, if not download
        if [ ! -f "data/raw/water_advisories_historical.csv" ]; then
          echo "‚ö†Ô∏è  Water advisories CSV not found - will be skipped in collection"
          echo "   Download from: https://www.sac-isc.gc.ca/eng/1506514143353/1533317130660"
        fi

        # CWB CSV - check if exists
        if [ ! -f "data/raw/CWB_2021.csv" ]; then
          echo "‚ö†Ô∏è  Community Well-Being CSV not found - will be skipped in collection"
          echo "   Download from: https://www.sac-isc.gc.ca/eng/1419773101942/1419773233645"
        fi

        # Census boundaries shapefile - check if exists
        if [ ! -f "data/raw/lcsd000a21a_e.shp" ]; then
          echo "üì• Downloading Census Subdivision boundaries..."
          cd data/raw
          wget --timeout=60 --tries=3 -q https://www12.statcan.gc.ca/census-recensement/2021/geo/sip-pis/boundary-limites/files-fichiers/lcsd000a21a_e.zip
          if [ $? -eq 0 ]; then
            unzip -q lcsd000a21a_e.zip
            rm lcsd000a21a_e.zip
            cd ../..
            echo "‚úÖ Census boundaries downloaded"
          else
            cd ../..
            echo "‚ùå Failed to download census boundaries"
            exit 1
          fi
        else
          echo "‚úÖ Census boundaries already exist"
        fi

    - name: Set environment variables for collection
      run: |
        echo "Mode: ${{ github.event.inputs.mode }}"
        echo "Overwrite: ${{ github.event.inputs.overwrite }}"

        if [ "${{ github.event.inputs.mode }}" == "collect-all" ]; then
          echo "Setting up to collect ALL enabled datasets from registry"
          python3 << 'PYTHON_EOF'
        import os
        from ontario_data.datasets import get_enabled_datasets

        overwrite = "${{ github.event.inputs.overwrite }}" == "true"

        # Write env vars to GITHUB_ENV
        env_file = os.getenv('GITHUB_ENV')
        with open(env_file, 'a') as f:
            for dataset in get_enabled_datasets():
                skip_var = f'SKIP_{dataset.id.upper()}'
                collect_var = f'COLLECT_{dataset.id.upper()}'

                # Check if dataset should be skipped
                if not overwrite and os.getenv(skip_var) == 'true':
                    print(f'  ‚è≠Ô∏è  {dataset.id}: skipped (already exists)')
                    continue

                f.write(f'{collect_var}=true\n')
                print(f'  ‚úì {collect_var}=true')
        PYTHON_EOF

        elif [ "${{ github.event.inputs.mode }}" == "collect-selected" ]; then
          echo "Setting up to collect selected datasets"

          # Parse newline or comma-separated list
          datasets_input="${{ github.event.inputs.datasets }}"

          # Replace newlines with spaces and commas with spaces
          datasets_input=$(echo "$datasets_input" | tr '\n,' ' ')

          # Loop through each dataset ID
          for dataset_id in $datasets_input; do
            dataset_id=$(echo "$dataset_id" | xargs)  # trim whitespace
            if [ ! -z "$dataset_id" ]; then
              skip_var="SKIP_${dataset_id^^}"
              collect_var="COLLECT_${dataset_id^^}"

              # Check if dataset should be skipped
              if [ "${{ github.event.inputs.overwrite }}" != "true" ] && [ "${!skip_var}" == "true" ]; then
                echo "  ‚è≠Ô∏è  $dataset_id: skipped (already exists)"
                continue
              fi

              echo "${collect_var}=true" >> $GITHUB_ENV
              echo "  ‚úì $dataset_id"
            fi
          done
        fi

    - name: Run data collection
      run: |
        echo "=========================================="
        echo "RUNNING DATA COLLECTION"
        echo "=========================================="
        python3 collect_data.py
      env:
        EBIRD_API_KEY: ${{ secrets.EBIRD_API_KEY }}


    - name: Check after status
      if: always()
      run: |
        echo ""
        echo "=== DATA STATUS AFTER COLLECTION ==="
        # Run status check (always exits 0, just reports data status)
        python check_data_status.py || true
        echo "Status check completed - see output above for details"

    - name: Upload data artifacts
      # Upload artifacts even if there were partial failures (but not if job was cancelled)
      if: success() || failure()
      uses: actions/upload-artifact@v4
      with:
        name: ontario-data-${{ github.run_number }}
        path: |
          data/processed/**/*.geojson
          data/processed/**/*.json
          data/processed/**/*.csv
          data/processed/collection_report.json
          data/raw/**/*.shp
          data/raw/**/*.shx
          data/raw/**/*.dbf
          data/raw/**/*.prj
          data/raw/**/*.cpg
          data/raw/**/*.csv
          data_status.json
        retention-days: 30
        if-no-files-found: warn

    - name: Create collection summary
      if: always()
      run: |
        echo "## üì¶ Data Collection Results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Run:** #${{ github.run_number }}" >> $GITHUB_STEP_SUMMARY
        echo "**Status:** ${{ job.status }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        echo "### Collection Mode" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Mode:** ${{ github.event.inputs.mode }}" >> $GITHUB_STEP_SUMMARY
        if [ "${{ github.event.inputs.mode }}" == "collect-selected" ]; then
          echo "**Selected datasets:** ${{ github.event.inputs.datasets }}" >> $GITHUB_STEP_SUMMARY
        fi
        echo "" >> $GITHUB_STEP_SUMMARY

        if [ -f "data/processed/collection_report.json" ]; then
          echo "### Collection Results" >> $GITHUB_STEP_SUMMARY
          echo '```json' >> $GITHUB_STEP_SUMMARY
          cat data/processed/collection_report.json | python -m json.tool 2>/dev/null || cat data/processed/collection_report.json
          echo '```' >> $GITHUB_STEP_SUMMARY
        fi

        if [ -f "data_status.json" ]; then
          echo "### Final Data Status" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          python check_data_status.py 2>&1 | tail -20
          echo '```' >> $GITHUB_STEP_SUMMARY
        fi
