name: Collect Data and Upload to S3

# This workflow collects Ontario environmental datasets and uploads them to S3
# This replaces the GitHub artifacts approach for persistent, scalable storage

on:
  workflow_dispatch:
    inputs:
      datasets:
        description: 'Comma-separated dataset IDs (leave empty for all enabled)'
        required: false
        type: string
      force_upload:
        description: 'Force upload even if no changes detected'
        type: boolean
        default: false
  schedule:
    # Run weekly on Sundays at 2 AM UTC
    - cron: '0 2 * * 0'

env:
  S3_BUCKET: ontario-environmental-data
  S3_REGION: us-east-1
  S3_BASE_PATH: datasets

jobs:
  collect-data:
    name: Collect Environmental Datasets
    runs-on: ubuntu-latest
    timeout-minutes: 60

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"
        cache: 'pip'

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y gdal-bin libgdal-dev

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"

    - name: Configure dataset selection
      id: datasets
      run: |
        if [ -n "${{ github.event.inputs.datasets }}" ]; then
          # User specified datasets
          IFS=',' read -ra DATASETS <<< "${{ github.event.inputs.datasets }}"
          for dataset in "${DATASETS[@]}"; do
            dataset=$(echo "$dataset" | xargs)  # trim whitespace
            export "COLLECT_${dataset^^}=true"
            echo "COLLECT_${dataset^^}=true" >> $GITHUB_ENV
          done
        else
          # Collect all enabled datasets
          export COLLECT_WILLIAMS_TREATY_COMMUNITIES=true
          export COLLECT_ONTARIO_RESERVES=true
          export COLLECT_WILLIAMS_TREATY_RESERVES=true
          export COLLECT_CONSERVATION_AUTHORITIES=true
          export COLLECT_EBIRD=true
          export COLLECT_COMMUNITY_WELLBEING=true
          export COLLECT_FIRE_PERIMETERS=true
          export COLLECT_ONTARIO_MUNICIPALITIES=true
          export COLLECT_WATERSHEDS=true
          export COLLECT_ONTARIO_BOUNDARY=true

          # Add to env for next steps
          echo "COLLECT_WILLIAMS_TREATY_COMMUNITIES=true" >> $GITHUB_ENV
          echo "COLLECT_ONTARIO_RESERVES=true" >> $GITHUB_ENV
          echo "COLLECT_WILLIAMS_TREATY_RESERVES=true" >> $GITHUB_ENV
          echo "COLLECT_CONSERVATION_AUTHORITIES=true" >> $GITHUB_ENV
          echo "COLLECT_EBIRD=true" >> $GITHUB_ENV
          echo "COLLECT_COMMUNITY_WELLBEING=true" >> $GITHUB_ENV
          echo "COLLECT_FIRE_PERIMETERS=true" >> $GITHUB_ENV
          echo "COLLECT_ONTARIO_MUNICIPALITIES=true" >> $GITHUB_ENV
          echo "COLLECT_WATERSHEDS=true" >> $GITHUB_ENV
          echo "COLLECT_ONTARIO_BOUNDARY=true" >> $GITHUB_ENV
        fi

    - name: Collect datasets
      env:
        EBIRD_API_KEY: ${{ secrets.EBIRD_API_KEY }}
      run: |
        echo "üîÑ Collecting datasets..."
        python3 collect_data.py

    - name: Check collection results
      run: |
        echo "=== Collection Results ==="
        if [ -f "data/processed/collection_report.json" ]; then
          cat data/processed/collection_report.json | python -m json.tool
        else
          echo "‚ö†Ô∏è  No collection report found"
          exit 1
        fi

        echo ""
        echo "=== Processed Files ==="
        find data/processed -type f -name "*.geojson" -exec ls -lh {} \;

    - name: Upload datasets to S3
      if: success()
      env:
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        AWS_DEFAULT_REGION: ${{ env.S3_REGION }}
      run: |
        echo "‚òÅÔ∏è  Uploading datasets to S3..."

        # Install AWS CLI
        pip install awscli

        # Upload all processed GeoJSON files to S3
        # Organized by category subdirectories

        # Boundaries
        if [ -d "data/processed/boundaries" ]; then
          echo "Uploading boundaries..."
          aws s3 sync data/processed/boundaries/ \
            s3://${{ env.S3_BUCKET }}/${{ env.S3_BASE_PATH }}/boundaries/ \
            \
            --cache-control "public, max-age=3600" \
            --content-type "application/geo+json" \
            --exclude "*" --include "*.geojson"
        fi

        # Communities
        if [ -d "data/processed/communities" ]; then
          echo "Uploading communities..."
          aws s3 sync data/processed/communities/ \
            s3://${{ env.S3_BUCKET }}/${{ env.S3_BASE_PATH }}/communities/ \
            \
            --cache-control "public, max-age=3600" \
            --content-type "application/geo+json" \
            --exclude "*" --include "*.geojson"
        fi

        # Community well-being
        if [ -d "data/processed/cwb" ]; then
          echo "Uploading community well-being data..."
          aws s3 sync data/processed/cwb/ \
            s3://${{ env.S3_BUCKET }}/${{ env.S3_BASE_PATH }}/community/ \
            \
            --cache-control "public, max-age=3600" \
            --content-type "application/geo+json" \
            --exclude "*" --include "*.geojson"
        fi

        # Protected areas
        if [ -d "data/processed/protected_areas" ]; then
          echo "Uploading protected areas..."
          aws s3 sync data/processed/protected_areas/ \
            s3://${{ env.S3_BUCKET }}/${{ env.S3_BASE_PATH }}/protected_areas/ \
            \
            --cache-control "public, max-age=3600" \
            --content-type "application/geo+json" \
            --exclude "*" --include "*.geojson"
        fi

        # Biodiversity
        if [ -d "data/processed/biodiversity" ]; then
          echo "Uploading biodiversity data..."
          aws s3 sync data/processed/biodiversity/ \
            s3://${{ env.S3_BUCKET }}/${{ env.S3_BASE_PATH }}/biodiversity/ \
            \
            --cache-control "public, max-age=86400" \
            --content-type "application/geo+json" \
            --exclude "*" --include "*.geojson"
        fi

        # Environmental (watersheds, fire perimeters, etc.)
        # Check root level first
        for file in data/processed/*.geojson; do
          if [ -f "$file" ]; then
            filename=$(basename "$file")
            echo "Uploading $filename to environmental..."
            aws s3 cp "$file" \
              s3://${{ env.S3_BUCKET }}/${{ env.S3_BASE_PATH }}/environmental/$filename \
              \
              --cache-control "public, max-age=3600" \
              --content-type "application/geo+json"
          fi
        done

        # Upload collection report as metadata
        if [ -f "data/processed/collection_report.json" ]; then
          echo "Uploading collection report..."
          aws s3 cp data/processed/collection_report.json \
            s3://${{ env.S3_BUCKET }}/metadata/collection_report.json \
            \
            --cache-control "public, max-age=300" \
            --content-type "application/json"
        fi

        echo "‚úÖ Upload to S3 complete"

    - name: Generate and upload catalog
      if: success()
      env:
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        AWS_DEFAULT_REGION: ${{ env.S3_REGION }}
      run: |
        echo "üìã Generating data catalog from S3..."

        # Generate catalog by querying S3 directly
        python3 << 'CATALOG_PY'
        import json
        import subprocess
        from pathlib import Path
        from datetime import datetime

        # Import dataset registry to get metadata
        import sys
        sys.path.insert(0, '.')
        from ontario_data.datasets import DATASETS

        # Base S3 URL
        bucket = "${{ env.S3_BUCKET }}"
        base_url = f"https://{bucket}.s3.${{ env.S3_REGION }}.amazonaws.com"

        # Build catalog
        catalog = {
            "metadata": {
                "generated": datetime.utcnow().isoformat() + "Z",
                "source": "Ontario Environmental Data Registry",
                "storage": {
                    "provider": "AWS S3",
                    "bucket": bucket,
                    "region": "${{ env.S3_REGION }}",
                    "base_url": base_url
                }
            },
            "datasets": {}
        }

        # Build catalog from dataset registry, verifying each file exists in S3
        for dataset_id, dataset in DATASETS.items():
            if not dataset.enabled:
                print(f"Skipping disabled dataset: {dataset_id}")
                continue

            # Determine S3 path to check
            if dataset.is_static and dataset.s3_url:
                # For static datasets, extract S3 path from the s3_url
                s3_url = dataset.s3_url
                # Extract path after bucket URL
                s3_path = s3_url.replace(f"{base_url}/", "")
            elif dataset.output_path:
                # For dynamic datasets, construct S3 path from output_path
                filename = dataset.output_path.name
                s3_path = f"${{ env.S3_BASE_PATH }}/{dataset.category}/{filename}"
                s3_url = f"{base_url}/{s3_path}"
            else:
                print(f"Skipping dataset without s3_url or output_path: {dataset_id}")
                continue

            # Verify file exists in S3 and get size
            check_result = subprocess.run(
                ["aws", "s3", "ls", f"s3://{bucket}/{s3_path}"],
                capture_output=True,
                text=True
            )

            if check_result.returncode != 0:
                print(f"‚ö†Ô∏è  Dataset {dataset_id} not found in S3: s3://{bucket}/{s3_path}")
                continue

            # Parse file size from ls output
            size_bytes = 0
            if check_result.stdout.strip():
                parts = check_result.stdout.strip().split()
                if len(parts) >= 3:
                    size_bytes = int(parts[2])

            size_mb = round(size_bytes / 1024 / 1024, 2)

            # Try to download and count features (only for reasonably-sized files)
            count = None
            count_note = None
            if size_mb < 100:  # Only download files smaller than 100MB
                try:
                    temp_file = Path(f"/tmp/{dataset_id}.{dataset.output_format}")
                    subprocess.run(
                        ["aws", "s3", "cp", f"s3://{bucket}/{s3_path}", str(temp_file)],
                        check=True,
                        capture_output=True
                    )

                    # Read feature count for GeoJSON/JSON files
                    if dataset.output_format in ["geojson", "json"]:
                        with open(temp_file) as f:
                            data = json.load(f)
                            count = len(data.get("features", []))

                    temp_file.unlink()
                    print(f"  Counted {count} features")
                except Exception as e:
                    print(f"  Could not count features for {dataset_id}: {e}")
                    count_note = "Error counting features"
            else:
                count_note = "Large file"

            # File exists, add to catalog
            catalog_entry = {
                "id": dataset_id,
                "url": s3_url,
                "category": dataset.category,
                "format": dataset.output_format,
                "size_mb": size_mb,
                "last_updated": datetime.utcnow().isoformat() + "Z"
            }

            # Add count if available, otherwise add note
            if count is not None:
                catalog_entry["count"] = count
            elif count_note:
                catalog_entry["count_note"] = count_note

            catalog["datasets"][dataset_id] = catalog_entry
            print(f"‚úÖ Added dataset: {dataset_id} -> {s3_url}")

        # Write catalog
        catalog_path = Path("data/catalog.json")
        catalog_path.parent.mkdir(parents=True, exist_ok=True)
        with open(catalog_path, "w") as f:
            json.dump(catalog, f, indent=2)

        print(f"‚úÖ Generated catalog with {len(catalog['datasets'])} datasets")
        CATALOG_PY

        # Upload catalog to S3
        echo "Uploading catalog to S3..."
        aws s3 cp data/catalog.json \
          s3://${{ env.S3_BUCKET }}/catalog.json \
          \
          --cache-control "public, max-age=300" \
          --content-type "application/json"

        echo "‚úÖ Catalog uploaded"

    - name: Upload backup artifact
      if: success()
      uses: actions/upload-artifact@v4
      with:
        name: ontario-data-backup-${{ github.run_number }}
        path: |
          data/processed/**/*.geojson
          data/processed/collection_report.json
          data/catalog.json
        retention-days: 30
        compression-level: 9
        if-no-files-found: error

    - name: Create summary
      if: always()
      run: |
        echo "## ‚òÅÔ∏è S3 Data Upload Results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Bucket:** \`${{ env.S3_BUCKET }}\`" >> $GITHUB_STEP_SUMMARY
        echo "**Region:** ${{ env.S3_REGION }}" >> $GITHUB_STEP_SUMMARY
        echo "**Status:** ${{ job.status }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        if [ -f "data/catalog.json" ]; then
          echo "### üìã Data Catalog" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Catalog URL: https://${{ env.S3_BUCKET }}.s3.${{ env.S3_REGION }}.amazonaws.com/catalog.json" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Datasets Uploaded" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          python3 << 'SUMMARY_PY'
        import json
        from pathlib import Path

        catalog_path = Path("data/catalog.json")
        if catalog_path.exists():
            with open(catalog_path) as f:
                catalog = json.load(f)

            for dataset_id, info in catalog["datasets"].items():
                print(f"- **{dataset_id}**: {info['count']} features ({info['size_mb']} MB)")
                print(f"  - URL: {info['url']}")
        SUMMARY_PY
        else
          echo "‚ùå Catalog not generated" >> $GITHUB_STEP_SUMMARY
        fi

        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Next Steps" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "1. Data is now available at public S3 URLs" >> $GITHUB_STEP_SUMMARY
        echo "2. Update GitHub Pages to reference S3 URLs" >> $GITHUB_STEP_SUMMARY
        echo "3. Test data access from applications" >> $GITHUB_STEP_SUMMARY
