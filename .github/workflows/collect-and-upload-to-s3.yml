name: Collect Data and Upload to S3

# This workflow collects Ontario environmental datasets and uploads them to S3
# This replaces the GitHub artifacts approach for persistent, scalable storage

on:
  workflow_dispatch:
    inputs:
      datasets:
        description: 'Comma-separated dataset IDs (leave empty for all enabled)'
        required: false
        type: string
      force_upload:
        description: 'Force upload even if no changes detected'
        type: boolean
        default: false
  schedule:
    # Run weekly on Sundays at 2 AM UTC
    - cron: '0 2 * * 0'

env:
  S3_BUCKET: ontario-environmental-data
  S3_REGION: us-east-1
  S3_BASE_PATH: datasets

jobs:
  collect-data:
    name: Collect Environmental Datasets
    runs-on: ubuntu-latest
    timeout-minutes: 60

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"
        cache: 'pip'

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y gdal-bin libgdal-dev

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"

    - name: Configure dataset selection
      id: datasets
      run: |
        if [ -n "${{ github.event.inputs.datasets }}" ]; then
          # User specified datasets
          IFS=',' read -ra DATASETS <<< "${{ github.event.inputs.datasets }}"
          for dataset in "${DATASETS[@]}"; do
            dataset=$(echo "$dataset" | xargs)  # trim whitespace
            export "COLLECT_${dataset^^}=true"
            echo "COLLECT_${dataset^^}=true" >> $GITHUB_ENV
          done
        else
          # Collect all enabled datasets
          export COLLECT_WILLIAMS_TREATY_COMMUNITIES=true
          export COLLECT_ONTARIO_RESERVES=true
          export COLLECT_WILLIAMS_TREATY_RESERVES=true
          export COLLECT_CONSERVATION_AUTHORITIES=true
          export COLLECT_EBIRD=true
          export COLLECT_COMMUNITY_WELLBEING=true
          export COLLECT_FIRE_PERIMETERS=true
          export COLLECT_ONTARIO_MUNICIPALITIES=true
          export COLLECT_WATERSHEDS=true
          export COLLECT_ONTARIO_BOUNDARY=true

          # Add to env for next steps
          echo "COLLECT_WILLIAMS_TREATY_COMMUNITIES=true" >> $GITHUB_ENV
          echo "COLLECT_ONTARIO_RESERVES=true" >> $GITHUB_ENV
          echo "COLLECT_WILLIAMS_TREATY_RESERVES=true" >> $GITHUB_ENV
          echo "COLLECT_CONSERVATION_AUTHORITIES=true" >> $GITHUB_ENV
          echo "COLLECT_EBIRD=true" >> $GITHUB_ENV
          echo "COLLECT_COMMUNITY_WELLBEING=true" >> $GITHUB_ENV
          echo "COLLECT_FIRE_PERIMETERS=true" >> $GITHUB_ENV
          echo "COLLECT_ONTARIO_MUNICIPALITIES=true" >> $GITHUB_ENV
          echo "COLLECT_WATERSHEDS=true" >> $GITHUB_ENV
          echo "COLLECT_ONTARIO_BOUNDARY=true" >> $GITHUB_ENV
        fi

    - name: Collect datasets
      env:
        EBIRD_API_KEY: ${{ secrets.EBIRD_API_KEY }}
      run: |
        echo "üîÑ Collecting datasets..."
        python3 collect_data.py

    - name: Check collection results
      run: |
        echo "=== Collection Results ==="
        if [ -f "data/processed/collection_report.json" ]; then
          cat data/processed/collection_report.json | python -m json.tool
        else
          echo "‚ö†Ô∏è  No collection report found"
          exit 1
        fi

        echo ""
        echo "=== Processed Files ==="
        find data/processed -type f -name "*.geojson" -exec ls -lh {} \;

    - name: Upload datasets to S3
      if: success()
      env:
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        AWS_DEFAULT_REGION: ${{ env.S3_REGION }}
      run: |
        echo "‚òÅÔ∏è  Uploading datasets to S3..."

        # Install AWS CLI
        pip install awscli

        # Upload all processed GeoJSON files to S3
        # Organized by category subdirectories

        # Boundaries
        if [ -d "data/processed/boundaries" ]; then
          echo "Uploading boundaries..."
          aws s3 sync data/processed/boundaries/ \
            s3://${{ env.S3_BUCKET }}/${{ env.S3_BASE_PATH }}/boundaries/ \
            \
            --cache-control "public, max-age=3600" \
            --content-type "application/geo+json" \
            --exclude "*" --include "*.geojson"
        fi

        # Communities
        if [ -d "data/processed/communities" ]; then
          echo "Uploading communities..."
          aws s3 sync data/processed/communities/ \
            s3://${{ env.S3_BUCKET }}/${{ env.S3_BASE_PATH }}/communities/ \
            \
            --cache-control "public, max-age=3600" \
            --content-type "application/geo+json" \
            --exclude "*" --include "*.geojson"
        fi

        # Community well-being
        if [ -d "data/processed/cwb" ]; then
          echo "Uploading community well-being data..."
          aws s3 sync data/processed/cwb/ \
            s3://${{ env.S3_BUCKET }}/${{ env.S3_BASE_PATH }}/community/ \
            \
            --cache-control "public, max-age=3600" \
            --content-type "application/geo+json" \
            --exclude "*" --include "*.geojson"
        fi

        # Protected areas
        if [ -d "data/processed/protected_areas" ]; then
          echo "Uploading protected areas..."
          aws s3 sync data/processed/protected_areas/ \
            s3://${{ env.S3_BUCKET }}/${{ env.S3_BASE_PATH }}/protected_areas/ \
            \
            --cache-control "public, max-age=3600" \
            --content-type "application/geo+json" \
            --exclude "*" --include "*.geojson"
        fi

        # Biodiversity
        if [ -d "data/processed/biodiversity" ]; then
          echo "Uploading biodiversity data..."
          aws s3 sync data/processed/biodiversity/ \
            s3://${{ env.S3_BUCKET }}/${{ env.S3_BASE_PATH }}/biodiversity/ \
            \
            --cache-control "public, max-age=86400" \
            --content-type "application/geo+json" \
            --exclude "*" --include "*.geojson"
        fi

        # Environmental (watersheds, fire perimeters, etc.)
        # Check root level first
        for file in data/processed/*.geojson; do
          if [ -f "$file" ]; then
            filename=$(basename "$file")
            echo "Uploading $filename to environmental..."
            aws s3 cp "$file" \
              s3://${{ env.S3_BUCKET }}/${{ env.S3_BASE_PATH }}/environmental/$filename \
              \
              --cache-control "public, max-age=3600" \
              --content-type "application/geo+json"
          fi
        done

        # Upload collection report as metadata
        if [ -f "data/processed/collection_report.json" ]; then
          echo "Uploading collection report..."
          aws s3 cp data/processed/collection_report.json \
            s3://${{ env.S3_BUCKET }}/metadata/collection_report.json \
            \
            --cache-control "public, max-age=300" \
            --content-type "application/json"
        fi

        echo "‚úÖ Upload to S3 complete"

    - name: Generate and upload catalog
      if: success()
      env:
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        AWS_DEFAULT_REGION: ${{ env.S3_REGION }}
      run: |
        echo "üìã Generating data catalog from S3..."

        # Generate catalog by querying S3 directly
        python3 << 'CATALOG_PY'
        import json
        import subprocess
        from pathlib import Path
        from datetime import datetime

        # Import dataset registry to get metadata
        import sys
        sys.path.insert(0, '.')
        from ontario_data.datasets import DATASETS

        # Base S3 URL
        bucket = "${{ env.S3_BUCKET }}"
        base_url = f"https://{bucket}.s3.${{ env.S3_REGION }}.amazonaws.com"

        # Build catalog
        catalog = {
            "metadata": {
                "generated": datetime.utcnow().isoformat() + "Z",
                "source": "Ontario Environmental Data Registry",
                "storage": {
                    "provider": "AWS S3",
                    "bucket": bucket,
                    "region": "${{ env.S3_REGION }}",
                    "base_url": base_url
                }
            },
            "datasets": {}
        }

        # Get list of all GeoJSON files in S3
        print("Querying S3 for datasets...")
        result = subprocess.run(
            ["aws", "s3", "ls", f"s3://{bucket}/${{ env.S3_BASE_PATH }}/", "--recursive"],
            capture_output=True,
            text=True
        )

        if result.returncode != 0:
            print(f"Error listing S3 files: {result.stderr}")
            exit(1)

        # Parse S3 listing
        s3_files = {}
        for line in result.stdout.strip().split('\n'):
            if not line or not line.endswith('.geojson'):
                continue

            parts = line.split()
            if len(parts) < 4:
                continue

            # Format: date time size path
            size_bytes = int(parts[2])
            s3_path = parts[3]

            # Extract category and filename from path
            # Path format: datasets/category/filename.geojson
            path_parts = s3_path.split('/')
            if len(path_parts) >= 3 and path_parts[0] == "${{ env.S3_BASE_PATH }}":
                category = path_parts[1]
                filename = path_parts[2]

                s3_files[filename] = {
                    "category": category,
                    "size_bytes": size_bytes,
                    "s3_path": s3_path
                }

        print(f"Found {len(s3_files)} files in S3")

        # Match S3 files to dataset registry
        for dataset_id, dataset in DATASETS.items():
            if not dataset.enabled:
                continue

            if not dataset.output_path:
                continue

            filename = dataset.output_path.name

            if filename in s3_files:
                file_info = s3_files[filename]
                category = file_info["category"]
                size_mb = round(file_info["size_bytes"] / 1024 / 1024, 2)

                # Try to download and get feature count (only for small files)
                count = 0
                if size_mb < 50:  # Only download files smaller than 50MB
                    try:
                        # Download file temporarily
                        temp_file = Path(f"/tmp/{filename}")
                        subprocess.run(
                            ["aws", "s3", "cp", f"s3://{bucket}/{file_info['s3_path']}", str(temp_file)],
                            check=True,
                            capture_output=True
                        )

                        # Read feature count
                        with open(temp_file) as f:
                            data = json.load(f)
                            count = len(data.get("features", []))

                        temp_file.unlink()
                    except Exception as e:
                        print(f"Could not get feature count for {filename}: {e}")

                s3_url = f"{base_url}/${{ env.S3_BASE_PATH }}/{category}/{filename}"

                catalog["datasets"][dataset_id] = {
                    "id": dataset_id,
                    "url": s3_url,
                    "category": category,
                    "format": "geojson",
                    "size_mb": size_mb,
                    "count": count,
                    "last_updated": datetime.utcnow().isoformat() + "Z"
                }

        # Write catalog
        catalog_path = Path("data/catalog.json")
        catalog_path.parent.mkdir(parents=True, exist_ok=True)
        with open(catalog_path, "w") as f:
            json.dump(catalog, f, indent=2)

        print(f"‚úÖ Generated catalog with {len(catalog['datasets'])} datasets")
        CATALOG_PY

        # Upload catalog to S3
        echo "Uploading catalog to S3..."
        aws s3 cp data/catalog.json \
          s3://${{ env.S3_BUCKET }}/catalog.json \
          \
          --cache-control "public, max-age=300" \
          --content-type "application/json"

        echo "‚úÖ Catalog uploaded"

    - name: Upload backup artifact
      if: success()
      uses: actions/upload-artifact@v4
      with:
        name: ontario-data-backup-${{ github.run_number }}
        path: |
          data/processed/**/*.geojson
          data/processed/collection_report.json
          data/catalog.json
        retention-days: 30
        compression-level: 9
        if-no-files-found: error

    - name: Create summary
      if: always()
      run: |
        echo "## ‚òÅÔ∏è S3 Data Upload Results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Bucket:** \`${{ env.S3_BUCKET }}\`" >> $GITHUB_STEP_SUMMARY
        echo "**Region:** ${{ env.S3_REGION }}" >> $GITHUB_STEP_SUMMARY
        echo "**Status:** ${{ job.status }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        if [ -f "data/catalog.json" ]; then
          echo "### üìã Data Catalog" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Catalog URL: https://${{ env.S3_BUCKET }}.s3.${{ env.S3_REGION }}.amazonaws.com/catalog.json" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Datasets Uploaded" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          python3 << 'SUMMARY_PY'
        import json
        from pathlib import Path

        catalog_path = Path("data/catalog.json")
        if catalog_path.exists():
            with open(catalog_path) as f:
                catalog = json.load(f)

            for dataset_id, info in catalog["datasets"].items():
                print(f"- **{dataset_id}**: {info['count']} features ({info['size_mb']} MB)")
                print(f"  - URL: {info['url']}")
        SUMMARY_PY
        else
          echo "‚ùå Catalog not generated" >> $GITHUB_STEP_SUMMARY
        fi

        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Next Steps" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "1. Data is now available at public S3 URLs" >> $GITHUB_STEP_SUMMARY
        echo "2. Update GitHub Pages to reference S3 URLs" >> $GITHUB_STEP_SUMMARY
        echo "3. Test data access from applications" >> $GITHUB_STEP_SUMMARY
