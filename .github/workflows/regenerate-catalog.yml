name: Regenerate Catalog from S3

# This workflow regenerates the catalog.json by querying what's in S3
# Use this when you've manually uploaded new data to S3 and want to update the catalog

on:
  workflow_dispatch:

permissions:
  contents: read
  actions: write  # Required to trigger other workflows

env:
  S3_BUCKET: ontario-environmental-data
  S3_REGION: us-east-1
  S3_BASE_PATH: datasets

jobs:
  regenerate-catalog:
    name: Regenerate Catalog
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .

    - name: Generate catalog from S3
      env:
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        AWS_DEFAULT_REGION: ${{ env.S3_REGION }}
      run: |
        echo "ðŸ“‹ Generating data catalog from S3..."

        # Install AWS CLI
        pip install awscli

        # Generate catalog by querying S3 directly
        python3 << 'CATALOG_PY'
        import json
        import subprocess
        from pathlib import Path
        from datetime import datetime

        # Import dataset registry to get metadata
        import sys
        sys.path.insert(0, '.')
        from ontario_data.datasets import DATASETS

        # Base S3 URL
        bucket = "${{ env.S3_BUCKET }}"
        base_url = f"https://{bucket}.s3.${{ env.S3_REGION }}.amazonaws.com"

        # Build catalog
        catalog = {
            "metadata": {
                "generated": datetime.utcnow().isoformat() + "Z",
                "source": "Ontario Environmental Data Registry",
                "storage": {
                    "provider": "AWS S3",
                    "bucket": bucket,
                    "region": "${{ env.S3_REGION }}",
                    "base_url": base_url
                }
            },
            "datasets": {}
        }

        # Build catalog from dataset registry, verifying each file exists in S3
        for dataset_id, dataset in DATASETS.items():
            if not dataset.enabled:
                print(f"Skipping disabled dataset: {dataset_id}")
                continue

            # Determine S3 path to check
            if dataset.is_static and dataset.s3_url:
                # For static datasets, extract S3 path from the s3_url
                s3_url = dataset.s3_url
                # Extract path after bucket URL
                s3_path = s3_url.replace(f"{base_url}/", "")
            elif dataset.output_path:
                # For dynamic datasets, construct S3 path from output_path
                filename = dataset.output_path.name
                s3_path = f"${{ env.S3_BASE_PATH }}/{dataset.category}/{filename}"
                s3_url = f"{base_url}/{s3_path}"
            else:
                print(f"Skipping dataset without s3_url or output_path: {dataset_id}")
                continue

            # Verify file exists in S3 and get size
            check_result = subprocess.run(
                ["aws", "s3", "ls", f"s3://{bucket}/{s3_path}"],
                capture_output=True,
                text=True
            )

            if check_result.returncode != 0:
                print(f"âš ï¸  Dataset {dataset_id} not found in S3: s3://{bucket}/{s3_path}")
                continue

            # Parse file size from ls output
            size_bytes = 0
            if check_result.stdout.strip():
                parts = check_result.stdout.strip().split()
                if len(parts) >= 3:
                    size_bytes = int(parts[2])

            size_mb = round(size_bytes / 1024 / 1024, 2)

            # Try to download and count features (only for reasonably-sized files)
            count = None
            count_note = None
            if size_mb < 100:  # Only download files smaller than 100MB
                try:
                    temp_file = Path(f"/tmp/{dataset_id}.{dataset.output_format}")
                    subprocess.run(
                        ["aws", "s3", "cp", f"s3://{bucket}/{s3_path}", str(temp_file)],
                        check=True,
                        capture_output=True
                    )

                    # Read feature count for GeoJSON/JSON files
                    if dataset.output_format in ["geojson", "json"]:
                        with open(temp_file) as f:
                            data = json.load(f)
                            count = len(data.get("features", []))

                    temp_file.unlink()
                    print(f"  Counted {count} features")
                except Exception as e:
                    print(f"  Could not count features for {dataset_id}: {e}")
                    count_note = "Error counting features"
            else:
                count_note = "Large file"

            # File exists, add to catalog
            catalog_entry = {
                "id": dataset_id,
                "url": s3_url,
                "category": dataset.category,
                "format": dataset.output_format,
                "size_mb": size_mb,
                "last_updated": datetime.utcnow().isoformat() + "Z"
            }

            # Add count if available, otherwise add note
            if count is not None:
                catalog_entry["count"] = count
            elif count_note:
                catalog_entry["count_note"] = count_note

            catalog["datasets"][dataset_id] = catalog_entry
            print(f"âœ… Added dataset: {dataset_id} -> {s3_url}")

        # Write catalog
        catalog_path = Path("data/catalog.json")
        catalog_path.parent.mkdir(parents=True, exist_ok=True)
        with open(catalog_path, "w") as f:
            json.dump(catalog, f, indent=2)

        print(f"âœ… Generated catalog with {len(catalog['datasets'])} datasets")
        print("\nDatasets in catalog:")
        for dataset_id in sorted(catalog["datasets"].keys()):
            info = catalog["datasets"][dataset_id]
            count_str = info.get('count', info.get('count_note', 'unknown'))
            print(f"  - {dataset_id}: {count_str} features, {info['size_mb']} MB")
        CATALOG_PY

    - name: Upload catalog to S3
      env:
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        AWS_DEFAULT_REGION: ${{ env.S3_REGION }}
      run: |
        echo "â˜ï¸  Uploading catalog to S3..."
        aws s3 cp data/catalog.json \
          s3://${{ env.S3_BUCKET }}/catalog.json \
          --cache-control "public, max-age=300" \
          --content-type "application/json"

        echo "âœ… Catalog uploaded to S3"

    - name: Trigger GitHub Pages update
      run: |
        echo "ðŸ”„ Triggering GitHub Pages publish workflow..."
        gh workflow run "publish-from-s3.yml"
      env:
        GH_TOKEN: ${{ github.token }}

    - name: Create summary
      run: |
        echo "## ðŸ“‹ Catalog Regeneration Complete" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Catalog URL:** https://${{ env.S3_BUCKET }}.s3.${{ env.S3_REGION }}.amazonaws.com/catalog.json" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        if [ -f "data/catalog.json" ]; then
          echo "### Datasets" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          python3 << 'SUMMARY_PY'
        import json
        from pathlib import Path

        catalog_path = Path("data/catalog.json")
        if catalog_path.exists():
            with open(catalog_path) as f:
                catalog = json.load(f)

            print(f"**Total datasets:** {len(catalog['datasets'])}")
            print("")
            for dataset_id, info in sorted(catalog["datasets"].items()):
                count_str = info.get('count', info.get('count_note', 'unknown'))
                print(f"- **{dataset_id}**: {count_str} features ({info['size_mb']} MB)")
        SUMMARY_PY
        fi
