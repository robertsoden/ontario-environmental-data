name: Publish Data to GitHub Pages

on:
  # Run manually
  workflow_dispatch:

  # Run after data collection completes successfully
  workflow_run:
    workflows: ["Data Collection"]
    types:
      - completed

# Sets permissions of the GITHUB_TOKEN to allow deployment to GitHub Pages
permissions:
  contents: read
  pages: write
  id-token: write

# Allow only one concurrent deployment
concurrency:
  group: "pages"
  cancel-in-progress: false

jobs:
  # Build job
  build:
    runs-on: ubuntu-latest
    # Skip if the triggering workflow failed
    if: github.event_name == 'workflow_dispatch' || github.event.workflow_run.conclusion == 'success'
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Download latest data artifact
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            // Get recent workflow runs for Data Collection
            const runs = await github.rest.actions.listWorkflowRuns({
              owner: context.repo.owner,
              repo: context.repo.repo,
              workflow_id: 'data-collection.yml',
              status: 'completed',
              per_page: 10
            });

            if (runs.data.workflow_runs.length === 0) {
              core.setFailed('No completed Data Collection workflow runs found');
              return;
            }

            console.log(`Found ${runs.data.workflow_runs.length} completed runs`);

            // Find the most recent artifact
            for (const run of runs.data.workflow_runs) {
              const artifacts = await github.rest.actions.listWorkflowRunArtifacts({
                owner: context.repo.owner,
                repo: context.repo.repo,
                run_id: run.id,
              });

              const dataArtifact = artifacts.data.artifacts.find(a =>
                a.name.startsWith("ontario-data-")
              );

              if (dataArtifact) {
                console.log(`Found latest artifact: ${dataArtifact.name} from run ${run.run_number}`);

                const download = await github.rest.actions.downloadArtifact({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  artifact_id: dataArtifact.id,
                  archive_format: 'zip',
                });

                fs.writeFileSync('${{github.workspace}}/data-artifact.zip', Buffer.from(download.data));
                console.log('Downloaded latest data artifact');
                return;
              }
            }

            core.setFailed('No ontario-data artifacts found in recent runs');

      - name: Extract data artifact
        run: |
          echo "Extracting data artifact..."
          mkdir -p data/processed

          if [ -f data-artifact.zip ]; then
            unzip -o data-artifact.zip -d .
            echo ""
            echo "=== Data directory contents ==="
            find data -type f | sort
            echo ""
            echo "=== Data files by type ==="
            echo "GeoJSON files: $(find data -name "*.geojson" | wc -l)"
            echo "JSON files: $(find data -name "*.json" | wc -l)"
            echo "CSV files: $(find data -name "*.csv" | wc -l)"
          else
            echo "ERROR: data-artifact.zip not found"
            exit 1
          fi

      - name: Verify data exists
        run: |
          echo "Checking for data files..."
          if [ ! -d "data" ]; then
            mkdir -p data/processed
            echo "Created data directory structure"
          fi

          # Count data files
          data_file_count=$(find data -type f -name "*.geojson" -o -name "*.json" -o -name "*.csv" | wc -l)

          if [ "$data_file_count" -eq 0 ]; then
            echo "‚ö†Ô∏è  No data files found to publish"
            if [ "${{ github.event_name }}" == "workflow_run" ]; then
              echo "ERROR: Data Collection workflow completed but no artifacts were downloaded"
              exit 1
            else
              echo "WARNING: Manual trigger without data files - will create empty catalog"
            fi
          else
            echo "‚úÖ Found $data_file_count data files:"
            find data -type f \( -name "*.geojson" -o -name "*.json" -o -name "*.csv" \) | head -20
          fi

      - name: Setup Pages
        uses: actions/configure-pages@v4

      - name: Create initial index.html for data directory
        run: |
          cat > data/index.html << 'HTMLEOF'
          <!DOCTYPE html>
          <html>
          <head>
              <title>Ontario Environmental Data</title>
              <style>
                  body { font-family: Arial, sans-serif; margin: 40px; max-width: 1200px; }
                  h1 { color: #2c5282; }
                  h2 { color: #2c5282; margin-top: 30px; }
                  .info { background: #e6f3ff; padding: 15px; border-radius: 5px; margin: 20px 0; }
                  code { background: #f5f5f5; padding: 2px 5px; border-radius: 3px; }
                  details { margin: 10px 0; }
                  summary {
                      cursor: pointer;
                      font-weight: bold;
                      padding: 10px;
                      background: #f0f4f8;
                      border-radius: 5px;
                      user-select: none;
                  }
                  summary:hover { background: #e1e8ed; }
                  .dataset-content {
                      padding: 15px;
                      margin-left: 20px;
                      border-left: 3px solid #4a90e2;
                  }
                  .dataset-meta {
                      display: flex;
                      gap: 20px;
                      margin: 10px 0;
                  }
                  .meta-item {
                      background: #f9f9f9;
                      padding: 5px 10px;
                      border-radius: 3px;
                      font-size: 0.9em;
                  }
                  .category { color: #2563eb; }
                  .size { color: #059669; }
                  .format { color: #7c3aed; }
              </style>
          </head>
          <body>
              <h1>Ontario Environmental Data</h1>
              <div class="info">
                  <p><strong>Open Environmental Data Repository for Ontario</strong></p>
                  <p>This GitHub Pages site hosts processed environmental data files for Ontario.</p>
                  <p>Base URL: <code>https://robertsoden.io/ontario-environmental-data/</code></p>
              </div>
              <h2>Data Catalog</h2>
              <ul>
                  <li><a href="catalog.json">catalog.json</a> - List of available datasets</li>
                  <li><a href="layers.json">layers.json</a> - MapLibre layer configuration (JSON)</li>
                  <li><a href="layers.yaml">layers.yaml</a> - MapLibre layer configuration (YAML)</li>
              </ul>

              <h2>Available Datasets</h2>
              <div id="datasets">
                  <!-- Datasets will be inserted here by the catalog generation script -->
              </div>

              <h2>Browse Data</h2>
              <ul>
                  <li><a href="processed/">processed/</a> - Processed datasets</li>
                  <li><a href="raw/">raw/</a> - Raw data files</li>
              </ul>
          </body>
          </html>
          HTMLEOF

      - name: Create directory structure
        run: |
          # Create expected directories even if they don't exist yet
          mkdir -p data/raw
          mkdir -p data/processed
          mkdir -p data/processed/boundaries
          mkdir -p data/processed/communities
          echo "Directory structure created"

      - name: Set up Python for catalog generation
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e "."
          pip install pyyaml

      - name: Generate data catalog and layer configurations
        run: |
          cat > generate_catalog.py << 'EOFPYTHON'
          import json
          import yaml
          from pathlib import Path
          from datetime import datetime
          import sys
          import os

          # Add current directory to Python path so we can import ontario_data
          sys.path.insert(0, os.getcwd())

          # Import the dataset registry
          from ontario_data.datasets import DATASETS

          # Default layer styling for different geometry types
          DEFAULT_STYLES = {
              "fill": {
                  "fill-color": "#4a90e2",
                  "fill-opacity": 0.3,
                  "fill-outline-color": "#2c5282"
              },
              "circle": {
                  "circle-color": "#e74c3c",
                  "circle-radius": 8,
                  "circle-stroke-width": 2,
                  "circle-stroke-color": "#ffffff"
              },
              "line": {
                  "line-color": "#4a90e2",
                  "line-width": 2
              }
          }

          # Infer layer type from output format and path
          def infer_layer_type(dataset):
              if dataset.output_format == "geojson":
                  # Try to infer from path or description
                  path_str = str(dataset.output_path).lower()
                  desc_str = dataset.description.lower()

                  if "point" in desc_str or "communit" in desc_str or "observation" in desc_str:
                      return "circle"
                  elif "line" in desc_str or "river" in desc_str or "road" in desc_str:
                      return "line"
                  else:
                      return "fill"  # Default for polygons
              elif dataset.output_format == "json":
                  return "circle"  # Assume observations are points
              return "fill"

          # Base URL for GitHub Pages
          BASE_URL = "https://robertsoden.io/ontario-environmental-data"

          # Scan for available datasets from the registry
          catalog = {
              "generated_at": datetime.now().isoformat(),
              "base_url": BASE_URL,
              "available_layers": [],
              "unavailable_layers": []
          }

          for dataset_id, dataset in DATASETS.items():
              # Only process enabled datasets that have output files
              if not dataset.enabled or not dataset.output_path:
                  continue

              file_path = Path(dataset.output_path)

              if file_path.exists():
                  # Dataset exists
                  file_size = file_path.stat().st_size
                  layer_type = infer_layer_type(dataset)

                  layer_info = {
                      "id": dataset_id,
                      "url": f"{BASE_URL}/{dataset.output_path}",
                      "local_path": str(dataset.output_path),
                      "name": dataset.name,
                      "description": dataset.description,
                      "category": dataset.category,
                      "type": dataset.output_format,
                      "layer_type": layer_type,
                      "size_bytes": file_size,
                      "size_human": f"{file_size / 1024:.1f} KB" if file_size < 1024*1024 else f"{file_size / (1024*1024):.1f} MB",
                      "paint": DEFAULT_STYLES.get(layer_type, {}),
                      "layout": {"visibility": "visible"}
                  }
                  catalog["available_layers"].append(layer_info)
              else:
                  # Dataset missing
                  catalog["unavailable_layers"].append({
                      "id": dataset_id,
                      "name": dataset.name,
                      "expected_path": str(dataset.output_path),
                      "description": dataset.description,
                      "category": dataset.category
                  })

          # Write catalog as JSON
          with open("data/catalog.json", "w") as f:
              json.dump(catalog, f, indent=2)

          # Generate YAML configuration for map
          map_config = {
              "version": "1.0",
              "data_source": BASE_URL,
              "layers": []
          }

          for layer in catalog["available_layers"]:
              map_config["layers"].append({
                  "id": layer["id"],
                  "name": layer["name"],
                  "description": layer["description"],
                  "source": {
                      "type": "geojson",
                      "data": layer["url"]
                  },
                  "type": layer["layer_type"],
                  "paint": layer["paint"],
                  "layout": layer["layout"]
              })

          # Write map configuration as YAML
          with open("data/layers.yaml", "w") as f:
              yaml.dump(map_config, f, default_flow_style=False, sort_keys=False)

          # Also write as JSON for easier consumption
          with open("data/layers.json", "w") as f:
              json.dump(map_config, f, indent=2)

          # Generate HTML for datasets list
          datasets_html = ""

          # Group by category
          from collections import defaultdict
          by_category = defaultdict(list)
          for layer in catalog['available_layers']:
              by_category[layer['category']].append(layer)

          # Sort categories
          category_order = ['boundaries', 'protected_areas', 'biodiversity', 'community', 'environmental']
          category_names = {
              'boundaries': 'Boundaries',
              'protected_areas': 'Protected Areas',
              'biodiversity': 'Biodiversity',
              'community': 'Community',
              'environmental': 'Environmental'
          }

          for cat in category_order:
              if cat in by_category:
                  datasets_html += f'<h3>{category_names.get(cat, cat.title())}</h3>\n'
                  for layer in sorted(by_category[cat], key=lambda x: x['name']):
                      datasets_html += f'''
          <details>
              <summary>{layer['name']}</summary>
              <div class="dataset-content">
                  <p>{layer['description']}</p>
                  <div class="dataset-meta">
                      <span class="meta-item category">Category: {layer['category']}</span>
                      <span class="meta-item size">Size: {layer['size_human']}</span>
                      <span class="meta-item format">Format: {layer['type']}</span>
                  </div>
                  <p><strong>Download:</strong> <a href="{layer['url']}">{layer['local_path']}</a></p>
              </div>
          </details>
          '''

          # Update index.html with datasets
          with open("data/index.html", "r") as f:
              html_content = f.read()

          html_content = html_content.replace(
              '<!-- Datasets will be inserted here by the catalog generation script -->',
              datasets_html
          )

          with open("data/index.html", "w") as f:
              f.write(html_content)

          # Print summary
          print(f"‚úì Found {len(catalog['available_layers'])} available datasets")
          print(f"‚úó Missing {len(catalog['unavailable_layers'])} expected datasets")
          print("\nAvailable layers:")
          for layer in catalog['available_layers']:
              print(f"  - {layer['name']} ({layer['size_human']})")
          if catalog['unavailable_layers']:
              print("\nUnavailable layers:")
              for layer in catalog['unavailable_layers']:
                  print(f"  - {layer['name']} (expected at {layer['expected_path']})")
          EOFPYTHON

          python generate_catalog.py

      - name: Generate directory index files
        run: |
          # Generate index.html for ALL subdirectories so directory browsing works
          find data -type d | sort | while read dir; do
            # Skip the root data directory (already has index.html)
            if [ "$dir" = "data" ]; then
              continue
            fi

            # Get relative path from data/
            rel_path="${dir#data/}"

            echo "Creating index for: $dir"

            cat > "$dir/index.html" << 'HTMLEOF'
          <!DOCTYPE html>
          <html>
          <head>
              <title>DIRECTORY_NAME - Ontario Environmental Data</title>
              <style>
                  body { font-family: Arial, sans-serif; margin: 40px; }
                  h1 { color: #2c5282; }
                  .file-list { list-style: none; padding: 0; }
                  .file-list li { padding: 8px; border-bottom: 1px solid #eee; }
                  .file-list a { text-decoration: none; color: #2563eb; }
                  .file-list a:hover { text-decoration: underline; }
                  .folder { color: #2563eb; font-weight: bold; }
                  .folder:before { content: "üìÅ "; }
                  .file:before { content: "üìÑ "; }
                  .size { color: #666; font-size: 0.9em; margin-left: 10px; }
                  .empty { color: #999; font-style: italic; }
                  .back { margin: 20px 0; }
              </style>
          </head>
          <body>
              <div class="back"><a href="../">‚Üê Back</a> | <a href="/ontario-environmental-data/">Home</a></div>
              <h1>DIRECTORY_NAME</h1>
              <ul class="file-list">
          HTMLEOF

            # Add subdirectory listings first
            has_content=0
            find "$dir" -maxdepth 1 -type d -not -path "$dir" | sort | while read subdir; do
              has_content=1
              subdirname=$(basename "$subdir")
              echo "                  <li class=\"folder\"><a href=\"$subdirname/\">$subdirname/</a></li>" >> "$dir/index.html"
            done

            # Add file listings
            find "$dir" -maxdepth 1 -type f -not -name "index.html" | sort | while read file; do
              has_content=1
              filename=$(basename "$file")
              filesize=$(du -h "$file" | cut -f1)
              echo "                  <li class=\"file\"><a href=\"$filename\">$filename</a><span class=\"size\">($filesize)</span></li>" >> "$dir/index.html"
            done

            # If no files or subdirectories, show empty message
            if [ "$(find "$dir" -maxdepth 1 -type f -not -name "index.html" | wc -l)" -eq 0 ] && \
               [ "$(find "$dir" -maxdepth 1 -type d -not -path "$dir" | wc -l)" -eq 0 ]; then
              echo "                  <li class=\"empty\">This directory is empty</li>" >> "$dir/index.html"
            fi

            # Close HTML
            cat >> "$dir/index.html" << 'HTMLEOF'
              </ul>
          </body>
          </html>
          HTMLEOF

            # Replace DIRECTORY_NAME with actual directory name
            sed -i "s|DIRECTORY_NAME|$rel_path|g" "$dir/index.html"
          done

          echo "Generated directory indexes:"
          find data -name "index.html" -type f

      - name: Upload artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: 'data'

  # Deployment job
  deploy:
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    runs-on: ubuntu-latest
    needs: build
    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
