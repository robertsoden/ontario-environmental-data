name: Publish Data to GitHub Pages

on:
  # Run manually
  workflow_dispatch:

  # Run after data collection completes successfully
  workflow_run:
    workflows: ["Data Collection"]
    types:
      - completed

# Sets permissions of the GITHUB_TOKEN to allow deployment to GitHub Pages
permissions:
  contents: read
  pages: write
  id-token: write

# Allow only one concurrent deployment
concurrency:
  group: "pages"
  cancel-in-progress: false

jobs:
  # Build job
  build:
    runs-on: ubuntu-latest
    # Skip if the triggering workflow failed
    if: github.event_name == 'workflow_dispatch' || github.event.workflow_run.conclusion == 'success'
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Download artifacts from Data Collection workflow
        if: github.event_name == 'workflow_run'
        uses: actions/github-script@v7
        with:
          script: |
            const artifacts = await github.rest.actions.listWorkflowRunArtifacts({
              owner: context.repo.owner,
              repo: context.repo.repo,
              run_id: context.payload.workflow_run.id,
            });

            if (artifacts.data.artifacts.length === 0) {
              core.setFailed('No artifacts found from Data Collection workflow');
              return;
            }

            // Download the most recent data artifact
            const dataArtifact = artifacts.data.artifacts.find(artifact =>
              artifact.name.startsWith('ontario-data-')
            );

            if (!dataArtifact) {
              core.setFailed('No ontario-data artifact found');
              return;
            }

            console.log(`Downloading artifact: ${dataArtifact.name}`);

            const download = await github.rest.actions.downloadArtifact({
              owner: context.repo.owner,
              repo: context.repo.repo,
              artifact_id: dataArtifact.id,
              archive_format: 'zip',
            });

            const fs = require('fs');
            fs.writeFileSync('artifact.zip', Buffer.from(download.data));

      - name: Extract artifacts
        if: github.event_name == 'workflow_run'
        run: |
          if [ -f artifact.zip ]; then
            echo "Extracting artifact.zip..."
            unzip -o artifact.zip -d data/processed/
            echo "Artifact contents:"
            find data/processed -type f
          else
            echo "No artifact.zip found - using existing data"
          fi

      - name: Verify data exists
        run: |
          echo "Checking for data files..."
          if [ ! -d "data" ] || [ -z "$(find data -type f)" ]; then
            echo "ERROR: No data files found to publish!"
            echo "Please run the Data Collection workflow first or ensure data exists in the repository."
            exit 1
          fi
          echo "Found data files:"
          find data -type f | head -20

      - name: Setup Pages
        uses: actions/configure-pages@v4

      - name: Create index.html for data directory
        run: |
          cat > data/index.html << 'HTMLEOF'
          <!DOCTYPE html>
          <html>
          <head>
              <title>Ontario Environmental Data</title>
              <style>
                  body { font-family: Arial, sans-serif; margin: 40px; }
                  h1 { color: #2c5282; }
                  .info { background: #e6f3ff; padding: 15px; border-radius: 5px; margin: 20px 0; }
                  code { background: #f5f5f5; padding: 2px 5px; border-radius: 3px; }
              </style>
          </head>
          <body>
              <h1>Ontario Environmental Data</h1>
              <div class="info">
                  <p><strong>Open Environmental Data Repository for Ontario</strong></p>
                  <p>This GitHub Pages site hosts processed environmental data files for Ontario.</p>
                  <p>Base URL: <code>https://robertsoden.github.io/ontario-environmental-data/</code></p>
              </div>
              <h2>Data Catalog</h2>
              <ul>
                  <li><a href="catalog.json">catalog.json</a> - List of available datasets</li>
                  <li><a href="layers.json">layers.json</a> - MapLibre layer configuration (JSON)</li>
                  <li><a href="layers.yaml">layers.yaml</a> - MapLibre layer configuration (YAML)</li>
              </ul>
              <h2>Browse Data</h2>
              <ul>
                  <li><a href="processed/">processed/</a> - Processed datasets</li>
                  <li><a href="raw/">raw/</a> - Raw data files</li>
              </ul>
          </body>
          </html>
          HTMLEOF

      - name: Create directory structure
        run: |
          # Create expected directories even if they don't exist yet
          mkdir -p data/raw
          mkdir -p data/processed
          mkdir -p data/processed/boundaries
          mkdir -p data/processed/communities
          echo "Directory structure created"

      - name: Set up Python for catalog generation
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e "."
          pip install pyyaml

      - name: Generate data catalog and layer configurations
        run: |
          cat > generate_catalog.py << 'EOFPYTHON'
          import json
          import yaml
          from pathlib import Path
          from datetime import datetime
          import sys
          import os

          # Add current directory to Python path so we can import ontario_data
          sys.path.insert(0, os.getcwd())

          # Import the dataset registry
          from ontario_data.datasets import DATASETS

          # Default layer styling for different geometry types
          DEFAULT_STYLES = {
              "fill": {
                  "fill-color": "#4a90e2",
                  "fill-opacity": 0.3,
                  "fill-outline-color": "#2c5282"
              },
              "circle": {
                  "circle-color": "#e74c3c",
                  "circle-radius": 8,
                  "circle-stroke-width": 2,
                  "circle-stroke-color": "#ffffff"
              },
              "line": {
                  "line-color": "#4a90e2",
                  "line-width": 2
              }
          }

          # Infer layer type from output format and path
          def infer_layer_type(dataset):
              if dataset.output_format == "geojson":
                  # Try to infer from path or description
                  path_str = str(dataset.output_path).lower()
                  desc_str = dataset.description.lower()

                  if "point" in desc_str or "communit" in desc_str or "observation" in desc_str:
                      return "circle"
                  elif "line" in desc_str or "river" in desc_str or "road" in desc_str:
                      return "line"
                  else:
                      return "fill"  # Default for polygons
              elif dataset.output_format == "json":
                  return "circle"  # Assume observations are points
              return "fill"

          # Base URL for GitHub Pages
          BASE_URL = "https://robertsoden.io/ontario-environmental-data"

          # Scan for available datasets from the registry
          catalog = {
              "generated_at": datetime.now().isoformat(),
              "base_url": BASE_URL,
              "available_layers": [],
              "unavailable_layers": []
          }

          for dataset_id, dataset in DATASETS.items():
              # Only process enabled datasets that have output files
              if not dataset.enabled or not dataset.output_path:
                  continue

              file_path = Path(dataset.output_path)

              if file_path.exists():
                  # Dataset exists
                  file_size = file_path.stat().st_size
                  layer_type = infer_layer_type(dataset)

                  layer_info = {
                      "id": dataset_id,
                      "url": f"{BASE_URL}/{dataset.output_path}",
                      "local_path": str(dataset.output_path),
                      "name": dataset.name,
                      "description": dataset.description,
                      "category": dataset.category,
                      "type": dataset.output_format,
                      "layer_type": layer_type,
                      "size_bytes": file_size,
                      "size_human": f"{file_size / 1024:.1f} KB" if file_size < 1024*1024 else f"{file_size / (1024*1024):.1f} MB",
                      "paint": DEFAULT_STYLES.get(layer_type, {}),
                      "layout": {"visibility": "visible"}
                  }
                  catalog["available_layers"].append(layer_info)
              else:
                  # Dataset missing
                  catalog["unavailable_layers"].append({
                      "id": dataset_id,
                      "name": dataset.name,
                      "expected_path": str(dataset.output_path),
                      "description": dataset.description,
                      "category": dataset.category
                  })

          # Write catalog as JSON
          with open("data/catalog.json", "w") as f:
              json.dump(catalog, f, indent=2)

          # Generate YAML configuration for map
          map_config = {
              "version": "1.0",
              "data_source": BASE_URL,
              "layers": []
          }

          for layer in catalog["available_layers"]:
              map_config["layers"].append({
                  "id": layer["id"],
                  "name": layer["name"],
                  "description": layer["description"],
                  "source": {
                      "type": "geojson",
                      "data": layer["url"]
                  },
                  "type": layer["layer_type"],
                  "paint": layer["paint"],
                  "layout": layer["layout"]
              })

          # Write map configuration as YAML
          with open("data/layers.yaml", "w") as f:
              yaml.dump(map_config, f, default_flow_style=False, sort_keys=False)

          # Also write as JSON for easier consumption
          with open("data/layers.json", "w") as f:
              json.dump(map_config, f, indent=2)

          # Print summary
          print(f"‚úì Found {len(catalog['available_layers'])} available datasets")
          print(f"‚úó Missing {len(catalog['unavailable_layers'])} expected datasets")
          print("\nAvailable layers:")
          for layer in catalog['available_layers']:
              print(f"  - {layer['name']} ({layer['size_human']})")
          if catalog['unavailable_layers']:
              print("\nUnavailable layers:")
              for layer in catalog['unavailable_layers']:
                  print(f"  - {layer['name']} (expected at {layer['expected_path']})")
          EOFPYTHON

          python generate_catalog.py

      - name: Generate directory index files
        run: |
          # Generate index.html for ALL subdirectories so directory browsing works
          find data -type d | sort | while read dir; do
            # Skip the root data directory (already has index.html)
            if [ "$dir" = "data" ]; then
              continue
            fi

            # Get relative path from data/
            rel_path="${dir#data/}"

            echo "Creating index for: $dir"

            cat > "$dir/index.html" << 'HTMLEOF'
          <!DOCTYPE html>
          <html>
          <head>
              <title>DIRECTORY_NAME - Ontario Environmental Data</title>
              <style>
                  body { font-family: Arial, sans-serif; margin: 40px; }
                  h1 { color: #2c5282; }
                  .file-list { list-style: none; padding: 0; }
                  .file-list li { padding: 8px; border-bottom: 1px solid #eee; }
                  .file-list a { text-decoration: none; color: #2563eb; }
                  .file-list a:hover { text-decoration: underline; }
                  .folder { color: #2563eb; font-weight: bold; }
                  .folder:before { content: "üìÅ "; }
                  .file:before { content: "üìÑ "; }
                  .size { color: #666; font-size: 0.9em; margin-left: 10px; }
                  .empty { color: #999; font-style: italic; }
                  .back { margin: 20px 0; }
              </style>
          </head>
          <body>
              <div class="back"><a href="../">‚Üê Back</a> | <a href="/ontario-environmental-data/">Home</a></div>
              <h1>DIRECTORY_NAME</h1>
              <ul class="file-list">
          HTMLEOF

            # Add subdirectory listings first
            has_content=0
            find "$dir" -maxdepth 1 -type d -not -path "$dir" | sort | while read subdir; do
              has_content=1
              subdirname=$(basename "$subdir")
              echo "                  <li class=\"folder\"><a href=\"$subdirname/\">$subdirname/</a></li>" >> "$dir/index.html"
            done

            # Add file listings
            find "$dir" -maxdepth 1 -type f -not -name "index.html" | sort | while read file; do
              has_content=1
              filename=$(basename "$file")
              filesize=$(du -h "$file" | cut -f1)
              echo "                  <li class=\"file\"><a href=\"$filename\">$filename</a><span class=\"size\">($filesize)</span></li>" >> "$dir/index.html"
            done

            # If no files or subdirectories, show empty message
            if [ "$(find "$dir" -maxdepth 1 -type f -not -name "index.html" | wc -l)" -eq 0 ] && \
               [ "$(find "$dir" -maxdepth 1 -type d -not -path "$dir" | wc -l)" -eq 0 ]; then
              echo "                  <li class=\"empty\">This directory is empty</li>" >> "$dir/index.html"
            fi

            # Close HTML
            cat >> "$dir/index.html" << 'HTMLEOF'
              </ul>
          </body>
          </html>
          HTMLEOF

            # Replace DIRECTORY_NAME with actual directory name
            sed -i "s|DIRECTORY_NAME|$rel_path|g" "$dir/index.html"
          done

          echo "Generated directory indexes:"
          find data -name "index.html" -type f

      - name: Upload artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: 'data'

  # Deployment job
  deploy:
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    runs-on: ubuntu-latest
    needs: build
    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
