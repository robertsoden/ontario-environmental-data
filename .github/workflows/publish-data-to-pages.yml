name: Publish Data to GitHub Pages

on:
  # Run manually
  workflow_dispatch:

  # Run after data collection completes successfully
  workflow_run:
    workflows: ["Data Collection"]
    types:
      - completed

# Sets permissions of the GITHUB_TOKEN to allow deployment to GitHub Pages
permissions:
  contents: read
  pages: write
  id-token: write

# Allow only one concurrent deployment
concurrency:
  group: "pages"
  cancel-in-progress: false

jobs:
  # Build job
  build:
    runs-on: ubuntu-latest
    # Skip if the triggering workflow failed
    if: github.event_name == 'workflow_dispatch' || github.event.workflow_run.conclusion == 'success'
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Download artifacts from Data Collection workflow
        if: github.event_name == 'workflow_run'
        uses: actions/github-script@v7
        with:
          script: |
            const artifacts = await github.rest.actions.listWorkflowRunArtifacts({
              owner: context.repo.owner,
              repo: context.repo.repo,
              run_id: context.payload.workflow_run.id,
            });

            if (artifacts.data.artifacts.length === 0) {
              core.setFailed('No artifacts found from Data Collection workflow');
              return;
            }

            // Download the most recent data artifact
            const dataArtifact = artifacts.data.artifacts.find(artifact =>
              artifact.name.startsWith('ontario-data-')
            );

            if (!dataArtifact) {
              core.setFailed('No ontario-data artifact found');
              return;
            }

            console.log(`Downloading artifact: ${dataArtifact.name}`);

            const download = await github.rest.actions.downloadArtifact({
              owner: context.repo.owner,
              repo: context.repo.repo,
              artifact_id: dataArtifact.id,
              archive_format: 'zip',
            });

            const fs = require('fs');
            fs.writeFileSync('artifact.zip', Buffer.from(download.data));

      - name: Extract artifacts
        if: github.event_name == 'workflow_run'
        run: |
          if [ -f artifact.zip ]; then
            echo "Extracting artifact.zip..."
            unzip -o artifact.zip -d data/processed/
            echo "Artifact contents:"
            find data/processed -type f
          else
            echo "No artifact.zip found - using existing data"
          fi

      - name: Verify data exists
        run: |
          echo "Checking for data files..."
          if [ ! -d "data" ] || [ -z "$(find data -type f)" ]; then
            echo "ERROR: No data files found to publish!"
            echo "Please run the Data Collection workflow first or ensure data exists in the repository."
            exit 1
          fi
          echo "Found data files:"
          find data -type f | head -20

      - name: Setup Pages
        uses: actions/configure-pages@v4

      - name: Create index.html for data directory
        run: |
          # Create a simple index.html that lists available data
          cat > data/index.html << 'EOF'
          <!DOCTYPE html>
          <html>
          <head>
              <title>Ontario Environmental Data</title>
              <style>
                  body { font-family: Arial, sans-serif; margin: 40px; }
                  h1 { color: #2c5282; }
                  .info { background: #e6f3ff; padding: 15px; border-radius: 5px; margin: 20px 0; }
                  code { background: #f5f5f5; padding: 2px 5px; border-radius: 3px; }
              </style>
          </head>
          <body>
              <h1>Ontario Environmental Data</h1>
              <div class="info">
                  <p><strong>Open Environmental Data Repository for Ontario</strong></p>
                  <p>This GitHub Pages site hosts processed environmental data files for Ontario, including treaty territories, parks, fire history, conservation areas, and biodiversity observations.</p>
                  <p>Base URL: <code>https://robertsoden.github.io/ontario-environmental-data/</code></p>
              </div>

              <h2>Available Data Directories</h2>
              <ul>
                  <li><a href="processed/">processed/</a> - Processed datasets ready for visualization</li>
                  <li><a href="raw/">raw/</a> - Raw source data</li>
                  <li><a href="processed/boundaries/">processed/boundaries/</a> - Boundary files</li>
              </ul>

              <h2>Data Catalog & Configuration</h2>
              <p>Use these files to integrate with the Williams Treaties map:</p>
              <ul>
                  <li><a href="catalog.json">catalog.json</a> - List of available and unavailable datasets</li>
                  <li><a href="layers.json">layers.json</a> - MapLibre layer configuration (JSON)</li>
                  <li><a href="layers.yaml">layers.yaml</a> - MapLibre layer configuration (YAML)</li>
              </ul>

              <h2>Usage</h2>
              <p>Access data files directly via URL:</p>
              <pre><code>https://robertsoden.github.io/ontario-environmental-data/processed/boundaries/williams_treaty.geojson</code></pre>

              <p>Or fetch the catalog programmatically:</p>
              <pre><code>fetch('https://robertsoden.io/ontario-environmental-data/catalog.json')
  .then(r => r.json())
  .then(data => console.log(data.available_layers))</code></pre>

              <h2>Related Projects</h2>
              <ul>
                  <li><a href="https://github.com/robertsoden/ontario-environmental-data">ontario-environmental-data</a> - Data generation repository</li>
                  <li><a href="https://github.com/robertsoden/williams-treaties">williams-treaties</a> - Interactive map visualization</li>
              </ul>
          </body>
          </html>
          EOF

      - name: Create directory structure
        run: |
          # Create expected directories even if they don't exist yet
          mkdir -p data/raw
          mkdir -p data/processed
          mkdir -p data/processed/boundaries
          mkdir -p data/processed/communities
          echo "Directory structure created"

      - name: Set up Python for catalog generation
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install PyYAML
        run: pip install pyyaml

      - name: Generate data catalog and layer configurations
        run: |
          cat > generate_catalog.py << 'EOFPYTHON'
          import json
          import yaml
          from pathlib import Path
          from datetime import datetime

          # Define expected datasets with metadata
          DATASETS = {
              "williams_treaty_boundaries": {
                  "path": "processed/boundaries/williams_treaty.geojson",
                  "type": "geojson",
                  "layer_type": "fill",
                  "name": "Williams Treaty Territories",
                  "description": "Boundaries of Williams Treaty First Nations territories",
                  "source": "Statistics Canada",
                  "paint": {
                      "fill-color": "#4a90e2",
                      "fill-opacity": 0.3,
                      "fill-outline-color": "#2c5282"
                  },
                  "layout": {"visibility": "visible"}
              },
              "williams_treaty_communities": {
                  "path": "processed/communities/williams_treaty_communities.geojson",
                  "type": "geojson",
                  "layer_type": "circle",
                  "name": "Williams Treaty Communities",
                  "description": "Community locations within Williams Treaty territories",
                  "source": "Statistics Canada",
                  "paint": {
                      "circle-color": "#e74c3c",
                      "circle-radius": 8,
                      "circle-stroke-width": 2,
                      "circle-stroke-color": "#ffffff"
                  },
                  "layout": {"visibility": "visible"}
              },
              "fire_perimeters": {
                  "path": "processed/fire_perimeters_1976_2024.geojson",
                  "type": "geojson",
                  "layer_type": "fill",
                  "name": "Fire Perimeters (1976-2024)",
                  "description": "Historical wildfire perimeters in Ontario",
                  "source": "Canadian Wildfire Information System",
                  "paint": {
                      "fill-color": "#ff6b6b",
                      "fill-opacity": 0.4,
                      "fill-outline-color": "#c92a2a"
                  },
                  "layout": {"visibility": "visible"}
              },
              "provincial_parks": {
                  "path": "processed/provincial_parks.geojson",
                  "type": "geojson",
                  "layer_type": "fill",
                  "name": "Provincial Parks",
                  "description": "Ontario provincial parks and protected areas",
                  "source": "Ontario GeoHub",
                  "paint": {
                      "fill-color": "#51cf66",
                      "fill-opacity": 0.3,
                      "fill-outline-color": "#2b8a3e"
                  },
                  "layout": {"visibility": "visible"}
              },
              "conservation_authorities": {
                  "path": "processed/conservation_authorities.geojson",
                  "type": "geojson",
                  "layer_type": "fill",
                  "name": "Conservation Authorities",
                  "description": "Conservation authority boundaries in Ontario",
                  "source": "Ontario GeoHub",
                  "paint": {
                      "fill-color": "#4dabf7",
                      "fill-opacity": 0.2,
                      "fill-outline-color": "#1971c2"
                  },
                  "layout": {"visibility": "visible"}
              },
              "inaturalist": {
                  "path": "processed/inaturalist_observations_2024.json",
                  "type": "json",
                  "layer_type": "circle",
                  "name": "iNaturalist Observations",
                  "description": "Biodiversity observations from iNaturalist",
                  "source": "iNaturalist",
                  "paint": {
                      "circle-color": "#a9e34b",
                      "circle-radius": 6,
                      "circle-stroke-width": 1,
                      "circle-stroke-color": "#5c940d"
                  },
                  "layout": {"visibility": "visible"}
              }
          }

          # Base URL for GitHub Pages
          BASE_URL = "https://robertsoden.io/ontario-environmental-data"

          # Scan for available datasets
          catalog = {
              "generated_at": datetime.now().isoformat(),
              "base_url": BASE_URL,
              "available_layers": [],
              "unavailable_layers": []
          }

          data_dir = Path("data")

          for layer_id, metadata in DATASETS.items():
              file_path = data_dir / metadata["path"]

              if file_path.exists():
                  # Dataset exists
                  file_size = file_path.stat().st_size
                  layer_info = {
                      "id": layer_id,
                      "url": f"{BASE_URL}/{metadata['path']}",
                      "local_path": metadata["path"],
                      "name": metadata["name"],
                      "description": metadata["description"],
                      "source": metadata["source"],
                      "type": metadata["type"],
                      "layer_type": metadata["layer_type"],
                      "size_bytes": file_size,
                      "size_human": f"{file_size / 1024:.1f} KB" if file_size < 1024*1024 else f"{file_size / (1024*1024):.1f} MB",
                      "paint": metadata.get("paint", {}),
                      "layout": metadata.get("layout", {})
                  }
                  catalog["available_layers"].append(layer_info)
              else:
                  # Dataset missing
                  catalog["unavailable_layers"].append({
                      "id": layer_id,
                      "name": metadata["name"],
                      "expected_path": metadata["path"],
                      "description": metadata["description"],
                      "source": metadata["source"]
                  })

          # Write catalog as JSON
          with open(data_dir / "catalog.json", "w") as f:
              json.dump(catalog, f, indent=2)

          # Generate YAML configuration for map
          map_config = {
              "version": "1.0",
              "data_source": BASE_URL,
              "layers": []
          }

          for layer in catalog["available_layers"]:
              map_config["layers"].append({
                  "id": layer["id"],
                  "name": layer["name"],
                  "description": layer["description"],
                  "source": {
                      "type": "geojson",
                      "data": layer["url"]
                  },
                  "type": layer["layer_type"],
                  "paint": layer["paint"],
                  "layout": layer["layout"]
              })

          # Write map configuration as YAML
          with open(data_dir / "layers.yaml", "w") as f:
              yaml.dump(map_config, f, default_flow_style=False, sort_keys=False)

          # Also write as JSON for easier consumption
          with open(data_dir / "layers.json", "w") as f:
              json.dump(map_config, f, indent=2)

          # Print summary
          print(f"‚úì Found {len(catalog['available_layers'])} available datasets")
          print(f"‚úó Missing {len(catalog['unavailable_layers'])} expected datasets")
          print("\nAvailable layers:")
          for layer in catalog['available_layers']:
              print(f"  - {layer['name']} ({layer['size_human']})")
          if catalog['unavailable_layers']:
              print("\nUnavailable layers:")
              for layer in catalog['unavailable_layers']:
                  print(f"  - {layer['name']} (expected at {layer['expected_path']})")
          EOFPYTHON

          python generate_catalog.py

      - name: Generate directory index files
        run: |
          # Generate index.html for ALL subdirectories so directory browsing works
          find data -type d | sort | while read dir; do
            # Skip the root data directory (already has index.html)
            if [ "$dir" = "data" ]; then
              continue
            fi

            # Get relative path from data/
            rel_path="${dir#data/}"

            echo "Creating index for: $dir"

            cat > "$dir/index.html" << 'HTMLEOF'
          <!DOCTYPE html>
          <html>
          <head>
              <title>DIRECTORY_NAME - Ontario Environmental Data</title>
              <style>
                  body { font-family: Arial, sans-serif; margin: 40px; }
                  h1 { color: #2c5282; }
                  .file-list { list-style: none; padding: 0; }
                  .file-list li { padding: 8px; border-bottom: 1px solid #eee; }
                  .file-list a { text-decoration: none; color: #2563eb; }
                  .file-list a:hover { text-decoration: underline; }
                  .folder { color: #2563eb; font-weight: bold; }
                  .folder:before { content: "üìÅ "; }
                  .file:before { content: "üìÑ "; }
                  .size { color: #666; font-size: 0.9em; margin-left: 10px; }
                  .empty { color: #999; font-style: italic; }
                  .back { margin: 20px 0; }
              </style>
          </head>
          <body>
              <div class="back"><a href="../">‚Üê Back</a> | <a href="/ontario-environmental-data/">Home</a></div>
              <h1>DIRECTORY_NAME</h1>
              <ul class="file-list">
          HTMLEOF

            # Add subdirectory listings first
            has_content=0
            find "$dir" -maxdepth 1 -type d -not -path "$dir" | sort | while read subdir; do
              has_content=1
              subdirname=$(basename "$subdir")
              echo "                  <li class=\"folder\"><a href=\"$subdirname/\">$subdirname/</a></li>" >> "$dir/index.html"
            done

            # Add file listings
            find "$dir" -maxdepth 1 -type f -not -name "index.html" | sort | while read file; do
              has_content=1
              filename=$(basename "$file")
              filesize=$(du -h "$file" | cut -f1)
              echo "                  <li class=\"file\"><a href=\"$filename\">$filename</a><span class=\"size\">($filesize)</span></li>" >> "$dir/index.html"
            done

            # If no files or subdirectories, show empty message
            if [ "$(find "$dir" -maxdepth 1 -type f -not -name "index.html" | wc -l)" -eq 0 ] && \
               [ "$(find "$dir" -maxdepth 1 -type d -not -path "$dir" | wc -l)" -eq 0 ]; then
              echo "                  <li class=\"empty\">This directory is empty</li>" >> "$dir/index.html"
            fi

            # Close HTML
            cat >> "$dir/index.html" << 'HTMLEOF'
              </ul>
          </body>
          </html>
          HTMLEOF

            # Replace DIRECTORY_NAME with actual directory name
            sed -i "s|DIRECTORY_NAME|$rel_path|g" "$dir/index.html"
          done

          echo "Generated directory indexes:"
          find data -name "index.html" -type f

      - name: Upload artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: 'data'

  # Deployment job
  deploy:
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    runs-on: ubuntu-latest
    needs: build
    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
